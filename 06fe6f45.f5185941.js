(window.webpackJsonp=window.webpackJsonp||[]).push([[5],{54:function(e,t,n){"use strict";n.r(t),n.d(t,"frontMatter",(function(){return i})),n.d(t,"metadata",(function(){return s})),n.d(t,"rightToc",(function(){return c})),n.d(t,"default",(function(){return d}));var a=n(2),r=n(6),o=(n(0),n(99)),i={id:"Intro",title:"Face Mask",sidebar_label:"Introduction"},s={unversionedId:"classification/mask/Intro",id:"classification/mask/Intro",isDocsHomePage:!1,title:"Face Mask",description:"In this project, the aim of is develop a demo of scientific  and intelligent tools that can be used in the public health sector.",source:"@site/docs/classification/mask/Intro.md",permalink:"/docusaurus/docs/classification/mask/Intro",editUrl:"https://github.com/facebook/docusaurus/edit/master/website/docs/classification/mask/Intro.md",sidebar_label:"Introduction",sidebar:"classification",previous:{title:"Insturctions",permalink:"/docusaurus/docs/classification/leaves/instructions"},next:{title:"Insturctions",permalink:"/docusaurus/docs/classification/mask/instructions"}},c=[{value:"Introduction",id:"introduction",children:[]},{value:"Models",id:"models",children:[]},{value:"Demo Code",id:"demo-code",children:[]},{value:"Dateset and analysis",id:"dateset-and-analysis",children:[]}],l={rightToc:c};function d(e){var t=e.components,n=Object(r.a)(e,["components"]);return Object(o.b)("wrapper",Object(a.a)({},l,n,{components:t,mdxType:"MDXLayout"}),Object(o.b)("p",null,"In this project, the aim of is develop a ",Object(o.b)("em",{parentName:"p"},"demo")," of scientific  and intelligent tools that can be used in the public health sector."),Object(o.b)("h3",{id:"introduction"},"Introduction"),Object(o.b)("p",null,"In this project we made use of deep learning and SVMss to identify and classify  images of people wearing mask.\nA Deep Learning driven method is used to identify whether the person is wearing mask or not. And a lightweight SVM is used for facial recognition."),Object(o.b)("h4",{id:"data"},"Data"),Object(o.b)("p",null,"For the Maask wearing identification part of this project, there are a total of 3835 images belonging to two classes:\n",Object(o.b)("strong",{parentName:"p"},Object(o.b)("em",{parentName:"strong"},"with_mask: 1916 images")),"\n",Object(o.b)("strong",{parentName:"p"},Object(o.b)("em",{parentName:"strong"},"without_mask: 1919 images"))),Object(o.b)("p",null,"For the facial recognition part, you will need at least 10~15 images per person of interest in your dataset."),Object(o.b)("h3",{id:"models"},"Models"),Object(o.b)("p",null,"In this project, the ",Object(o.b)("a",Object(a.a)({parentName:"p"},{href:"https://keras.io/"}),"Keras")," library was used along with ",Object(o.b)("a",Object(a.a)({parentName:"p"},{href:"https://opencv.org/f"}),"OpenCV")," to train the neural Network and the Support Vector Machine. The ",Object(o.b)("a",Object(a.a)({parentName:"p"},{href:"https://arxiv.org/abs/1704.04861"}),"MobileNet")," neural network was used for the Mask identification and it achieved an accuracy of ",Object(o.b)("strong",{parentName:"p"},Object(o.b)("em",{parentName:"strong"},"93%"))," on the task at hand."),Object(o.b)("h3",{id:"demo-code"},"Demo Code"),Object(o.b)("p",null,"The demo for this project can be done on live Video. The instruction on how to run the code will be under the instructions markedown file."),Object(o.b)("h3",{id:"dateset-and-analysis"},"Dateset and analysis"),Object(o.b)("p",null,"For each fish species, the data was randomly shuffled and a train and validation split was performed.\nThe dataset consists of very noisy and unclear pictures along with very well captured images.\nThe used dataset and was verified by experts in the aquatic domain ad therefore there are some pictures in the training set which belong to the wrong species of the fish."))}d.isMDXComponent=!0},99:function(e,t,n){"use strict";n.d(t,"a",(function(){return u})),n.d(t,"b",(function(){return f}));var a=n(0),r=n.n(a);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function s(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function c(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var l=r.a.createContext({}),d=function(e){var t=r.a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):s(s({},t),e)),n},u=function(e){var t=d(e.components);return r.a.createElement(l.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return r.a.createElement(r.a.Fragment,{},t)}},b=r.a.forwardRef((function(e,t){var n=e.components,a=e.mdxType,o=e.originalType,i=e.parentName,l=c(e,["components","mdxType","originalType","parentName"]),u=d(n),b=a,f=u["".concat(i,".").concat(b)]||u[b]||p[b]||o;return n?r.a.createElement(f,s(s({ref:t},l),{},{components:n})):r.a.createElement(f,s({ref:t},l))}));function f(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var o=n.length,i=new Array(o);i[0]=b;var s={};for(var c in t)hasOwnProperty.call(t,c)&&(s[c]=t[c]);s.originalType=e,s.mdxType="string"==typeof e?e:a,i[1]=s;for(var l=2;l<o;l++)i[l]=n[l];return r.a.createElement.apply(null,i)}return r.a.createElement.apply(null,n)}b.displayName="MDXCreateElement"}}]);